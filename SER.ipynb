{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\n",
    "DATA_PATH = '..//Speech Emotion Recognition//Radvess'\n",
    "SAMPLE_RATE = 48000\n",
    "\n",
    "# data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
    "data_list = []\n",
    "for dirname, _, filenames in os.walk(DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        identifiers = filename.split('.')[0].split('-')\n",
    "        emotion = (int(identifiers[2]))\n",
    "        if emotion == 8: # promeni surprise sa 8 na 0\n",
    "            emotion = 0\n",
    "        if int(identifiers[3]) == 1:\n",
    "            emotion_intensity = 'normal' \n",
    "        else:\n",
    "            emotion_intensity = 'strong'\n",
    "        if int(identifiers[6])%2 == 0:\n",
    "            gender = 'female'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "        \n",
    "        data_list.append({\"Emotion\": emotion,\n",
    "                            \"Emotion intensity\": emotion_intensity,\n",
    "                            \"Gender\": gender,\n",
    "                            \"Path\": file_path\n",
    "                             }\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_list)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "mel_spectrograms = []\n",
    "signals = []\n",
    "\n",
    "for i, file_path in enumerate(data.Path):\n",
    "    # Use soundfile to read audio data\n",
    "    audio, sample_rate = sf.read(file_path, dtype='float32')\n",
    "\n",
    "    # Convert stereo to mono by averaging channels\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "\n",
    "    # Limit duration and offset using array slicing\n",
    "    duration = 3 * sample_rate\n",
    "    offset = int(0.5 * sample_rate)\n",
    "    audio = audio[offset:offset + duration]\n",
    "\n",
    "    # Pad or truncate audio to ensure consistent length\n",
    "    signal = np.zeros((3 * sample_rate,), dtype='float32')\n",
    "    signal[:len(audio)] = audio\n",
    "\n",
    "    signals.append(signal)\n",
    "    print(\"\\rProcessed {}/{} files\".format(i + 1, len(data)), end='')\n",
    "\n",
    "signals = np.stack(signals, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, X_test = [], [], []\n",
    "Y_train, Y_val, Y_test = [], [], []\n",
    "\n",
    "for emotion in range(len(EMOTIONS)):\n",
    "    emotion_data = data[data['Emotion'] == emotion]\n",
    "    X, Y = [], []\n",
    "\n",
    "    for file_path in emotion_data['Path']:\n",
    "        try:\n",
    "            audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "            # Process audio data if needed (e.g., extracting features, resizing)\n",
    "            # For example, to resize audio to a fixed length:\n",
    "            audio = librosa.util.fix_length(audio, 3 * sample_rate)  # Adjust to desired length\n",
    "            X.append(audio)\n",
    "            Y.append(emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Split data for the current emotion into train, validation, and test sets (80-10-10 split)\n",
    "    X_train_temp, X_test_temp, Y_train_temp, Y_test_temp = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "    X_val_temp, X_test_temp, Y_val_temp, Y_test_temp = train_test_split(X_test_temp, Y_test_temp, test_size=0.5, random_state=42, stratify=Y_test_temp)\n",
    "\n",
    "    # Append the splits to respective lists\n",
    "    X_train.extend(X_train_temp)\n",
    "    Y_train.extend(Y_train_temp)\n",
    "    X_val.extend(X_val_temp)\n",
    "    Y_val.extend(Y_val_temp)\n",
    "    X_test.extend(X_test_temp)\n",
    "    Y_test.extend(Y_test_temp)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_val = np.array(X_val)\n",
    "Y_val = np.array(Y_val)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# Display shapes of train, validation, and test sets\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
    "    signal_len = len(signal)\n",
    "    \n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    \n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0 ** (num_bits - 1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    \n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    \n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    \n",
    "    # Compute K (scaling factor) for each noise instance\n",
    "    scaling_factors = np.sqrt((s_power / n_power) * 10 ** (-target_snr / 10))\n",
    "    scaled_noises = noise_norm * scaling_factors[:, np.newaxis]\n",
    "    \n",
    "    # Generate noisy signals by adding scaled noise to the original signal\n",
    "    noisy_signals = np.array([signal + scaled_noise * norm_constant for scaled_noise in scaled_noises])\n",
    "    \n",
    "    return noisy_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_signals = []\n",
    "aug_labels = []\n",
    "data_to_concat = []  # List to store rows to be concatenated\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i, :]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    aug_labels.extend([Y_train[i]] * augmented_signals.shape[0])  # Extend labels for augmented signals\n",
    "    aug_signals.extend(augmented_signals)  # Extend augmented signals\n",
    "    data_to_concat.extend([data.iloc[i]] * augmented_signals.shape[0])  # Extend corresponding data rows\n",
    "\n",
    "aug_signals = np.stack(aug_signals, axis=0)\n",
    "X_train_augmented = np.concatenate([X_train, aug_signals], axis=0)\n",
    "\n",
    "Y_train_augmented = np.array(aug_labels)  # Convert to numpy array\n",
    "\n",
    "# Concatenate the list of rows to create a new DataFrame\n",
    "data_augmented = pd.concat(data_to_concat, ignore_index=True)\n",
    "\n",
    "print('')\n",
    "print(f'X_train_augmented: {X_train_augmented.shape}, Y_train_augmented: {Y_train_augmented.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# test function\n",
    "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
    "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "signal[:len(audio)] = audio\n",
    "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMFCCs(audio, sample_rate, num_mfcc=13):\n",
    "    mfccs = librosa.feature.mfcc(y=audio,\n",
    "                                  sr=sample_rate,\n",
    "                                  n_mfcc=num_mfcc)\n",
    "    return mfccs\n",
    "\n",
    "# Test function\n",
    "audio, sample_rate = librosa.load(data.loc[0, 'Path'], duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
    "signal = np.zeros((int(SAMPLE_RATE * 3,)))\n",
    "signal[:len(audio)] = audio\n",
    "mfccs = getMFCCs(signal, SAMPLE_RATE)\n",
    "\n",
    "# Display the MFCCs\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time', sr=SAMPLE_RATE)\n",
    "plt.colorbar()\n",
    "plt.title('MFCCs')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('MFCCs shape:', mfccs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_mel_spectrogram(signal):\n",
    "    return getMELspectrogram(signal, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "# Calculate Mel spectrograms for train set\n",
    "print(\"Calculating mel spectrograms for train set\")\n",
    "mel_train = Parallel(n_jobs=-1)(delayed(calculate_mel_spectrogram)(signal) for signal in X_train)\n",
    "X_train = np.stack(mel_train, axis=0)\n",
    "del mel_train  # Clear memory\n",
    "\n",
    "# Calculate Mel spectrograms for validation set\n",
    "print(\"Calculating mel spectrograms for validation set\")\n",
    "mel_val = Parallel(n_jobs=-1)(delayed(calculate_mel_spectrogram)(signal) for signal in X_val)\n",
    "X_val = np.stack(mel_val, axis=0)\n",
    "del mel_val  # Clear memory\n",
    "\n",
    "# Calculate Mel spectrograms for test set\n",
    "print(\"Calculating mel spectrograms for test set\")\n",
    "mel_test = Parallel(n_jobs=-1)(delayed(calculate_mel_spectrogram)(signal) for signal in X_test)\n",
    "X_test = np.stack(mel_test, axis=0)\n",
    "del mel_test  # Clear memory\n",
    "\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                       out_channels=16,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 2. conv block\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                       out_channels=32,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 3. conv block\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 4. conv block\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        # Transformer block\n",
    "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        transf_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(320,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        # conv embedding\n",
    "        conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "        # transformer embedding\n",
    "        x_reduced = self.transf_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(2,0,1) # requires shape = (time,batch,embedding)\n",
    "        transf_out = self.transf_encoder(x_reduced)\n",
    "        transf_embedding = torch.mean(transf_out, dim=0)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([conv_embedding, transf_embedding], dim=1) \n",
    "        # final Linear\n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, device, epochs, dataset_size, batch_size):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_step(self, X, Y):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output_logits, output_softmax = self.model(X)\n",
    "        predictions = torch.argmax(output_softmax, dim=1)\n",
    "        accuracy = torch.sum(Y == predictions).item() / len(Y)\n",
    "\n",
    "        loss = self.loss_fn(output_logits, Y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), accuracy * 100\n",
    "\n",
    "    def validate(self, X, Y):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "\n",
    "            output_logits, output_softmax = self.model(X)\n",
    "            predictions = torch.argmax(output_softmax, dim=1)\n",
    "            accuracy = torch.sum(Y == predictions).item() / len(Y)\n",
    "\n",
    "            loss = self.loss_fn(output_logits, Y)\n",
    "\n",
    "        return loss.item(), accuracy * 100, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = np.expand_dims(X_train,1)\n",
    "X_val = np.expand_dims(X_val,1)\n",
    "X_test = np.expand_dims(X_test,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,c,h,w))\n",
    "\n",
    "b,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,c,h,w))\n",
    "\n",
    "b,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,c,h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1500\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Selected device is {}'.format(device))\n",
    "model = ParallelModel(num_emotions=len(EMOTIONS)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "# Create an instance of the Trainer class\n",
    "trainer = Trainer(model=model,\n",
    "                  loss_fn=loss_fnc,\n",
    "                  optimizer=OPTIMIZER,\n",
    "                  device=device,\n",
    "                  epochs=EPOCHS,\n",
    "                  dataset_size=DATASET_SIZE,\n",
    "                  batch_size=BATCH_SIZE)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Shuffle data indices\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind]\n",
    "    Y_train = Y_train[ind]\n",
    "    \n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end - batch_start\n",
    "        X = X_train[batch_start:batch_end]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X, device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long, device=device)\n",
    "        loss, acc = trainer.train_step(X_tensor, Y_tensor)\n",
    "        \n",
    "        epoch_acc += acc * actual_batch_size / DATASET_SIZE\n",
    "        epoch_loss += loss * actual_batch_size / DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\", end='')\n",
    "    \n",
    "    X_val_tensor = torch.tensor(X_val, device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val, dtype=torch.long, device=device)\n",
    "    val_loss, val_acc, predictions = trainer.validate(X_val_tensor, Y_val_tensor)\n",
    "    \n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Plotting losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'trained_model_state.pth')\n",
    "model.load_state_dict(torch.load('trained_model_state.pth'))  # Load the saved state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = trainer.validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "predicted_emotions = predictions.cpu().numpy()\n",
    "emotions_groundtruth = Y_test\n",
    "\n",
    "# build confusion matrix and normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(emotions_groundtruth, predicted_emotions)\n",
    "conf_matrix_norm = confusion_matrix(emotions_groundtruth, predicted_emotions,normalize='true')\n",
    "\n",
    "emotion_names = [emotion for emotion in EMOTIONS.values()]\n",
    "\n",
    "confmatrix_df = pd.DataFrame(conf_matrix, index=emotion_names, columns=emotion_names)\n",
    "confmatrix_df_norm = pd.DataFrame(conf_matrix_norm, index=emotion_names, columns=emotion_names)\n",
    "\n",
    "# plot confusion matrices\n",
    "plt.figure(figsize=(16,6))\n",
    "sn.set(font_scale=1.8) \n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df, annot=True, annot_kws={\"size\": 18}) \n",
    "plt.subplot(1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_strong = 0\n",
    "correct_normal = 0\n",
    "wrong_strong = 0\n",
    "wrong_normal = 0\n",
    "for i in range(len(X_test)):\n",
    "    intensity = data.loc[i,'Emotion intensity']\n",
    "    if Y_test[i] == predictions[i]: \n",
    "        if  intensity == 'normal':\n",
    "            correct_normal += 1\n",
    "        else:\n",
    "            correct_strong += 1\n",
    "    else: \n",
    "        if intensity == 'normal':\n",
    "            wrong_normal += 1\n",
    "        else:\n",
    "            wrong_strong += 1\n",
    "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
    "sn.set(font_scale=1.4) \n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_female = 0\n",
    "correct_male = 0\n",
    "wrong_female = 0\n",
    "wrong_male = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    gender = data.loc[i, 'Gender']\n",
    "    \n",
    "    if Y_test[i] == predictions[i]:  \n",
    "        if gender == 'female':\n",
    "            correct_female += 1\n",
    "        else:\n",
    "            correct_male += 1\n",
    "    else:  \n",
    "        if gender == 'female':\n",
    "            wrong_female += 1\n",
    "        else:\n",
    "            wrong_male += 1\n",
    "\n",
    "# Create a 2x2 array with the counts\n",
    "array = np.array([[wrong_female, wrong_male], [correct_female, correct_male]])\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame(array, ['wrong', 'correct'], ['female', 'male'])\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sn.set(font_scale=1.4)  \n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16})  \n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Correctness')\n",
    "plt.title('Correlation Matrix: Correctness vs Gender')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
